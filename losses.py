import torch
from typing import Union
from utils import generate_noise, generate_style_mixes

def wgan_loss(disc_fake : torch.Tensor, disc_real : torch.Tensor):
    """
    disc_fake: (batch_size, 1)
    disc_real: (batch_size, 1)

    Return sample WGAN-1 loss. When computed for critic, generator parameters should be detached from computational graph. By analogy, when computed for 
    generator, critic parameters should be detached from the computational graph.
    """
    return disc_fake.mean() - disc_real.mean()

def gradient_penalty(samples_pred : Union[torch.Tensor, None], samples : torch.Tensor, penalty_const : float = 10, fake_samples : Union[torch.Tensor, None] = None, critic = None):
    """
    Penalizes critic gradient deviations from 1, tries to enforce 1-Lipschitz constraint on the critic, as discussed in https://arxiv.org/pdf/1704.00028.
    Additionaly, previously mentioned paper that introduced gradient penalties computes gradients on samples that are obtained by linear interpolation
    between real and fake samples, but StyleGAN2 seems to compute this on real images only (https://github.com/rosinality/stylegan2-pytorch/blob/master/train.py#L71C5-L71C14).

    We implement gradient penalty so that it supports both regimes.
    """

    # Perform interpolation between real and fake samples, as discussed in WGAN-GP paper
    if fake_samples is not None:
        assert samples.shape == fake_samples.shape
        eps = torch.rand((fake_samples.shape[0], 1, 1, 1), device = fake_samples.get_device())
        samples = eps * samples + (1 - eps) * fake_samples
        samples_pred = critic(samples)

    # retain_graph should be true since we compute second order derivatives in backward pass.
    # When using MiniBatchSTD, prediction for every sample is affected by other samples as well, creating additional paths in computational graph.
    grad, = torch.autograd.grad(inputs = samples, outputs = samples_pred.sum(), create_graph = True) 
    # grad takes the same shape as images, it contains partial derivatives of images_pred with respect to elements of images
    norm = torch.sqrt(torch.sum(grad ** 2, dim = (1, 2, 3)))
    return penalty_const * ((norm - 1) ** 2).mean()

class PathLengthPenalty(torch.nn.Module):
    def __init__(self, beta = 0.99):
        super().__init__()
        self.beta = beta
        self.steps = 0
        self.a = 0.0
    
    def forward(self, w : torch.Tensor, x : torch.Tensor):
        """
        w: (batch_size, latent_dim). It needs to be expanded before being passed to the generator because of generalized style mixing forward pass.
        Should be covered by latest changes in the generator.

        x: (batch_size, channels, h, w) - Images generated by generator through w
        """
        rh, rw = x.shape[2], x.shape[3]
        device = x.device
        """
        Following is not mentioned in the paper but is present in StyleGAN2-ADA implementation.
        StyleGAN2 TensorFlow implementation: https://github.com/NVlabs/stylegan2/blob/master/training/loss.py#L167
        StyleGAN2-ADA implementation: https://github.com/NVlabs/stylegan2-ada-pytorch/blob/main/training/loss.py#L81C17-L81C26

        If x is zero mean unit variance per channel, then this ensures unit variance per channel after computing the dot product.
        """

        y = torch.randn(x.shape, device = device) / ((rh * rw) ** 0.5)
        print((x * y).std(dim = (-1, -2)))
        out = (x * y).sum()
        grad, = torch.autograd.grad(inputs = w, outputs = out, create_graph = True)
        print(grad.shape)
        gnorm = torch.norm(grad, dim = -1)
        return ((gnorm - self.a) ** 2).mean()